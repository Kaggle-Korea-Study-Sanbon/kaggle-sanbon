{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec 03. Linear Regression 의 cost 최소화 Tensorflow 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified hypothesis 에 대해 cost function 모양은?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [1,2,3]\n",
    "y = [1,2,3]\n",
    "\n",
    "W = tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis for linear model W * X\n",
    "hypothesis = W * X\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initialize global variables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Variables for plotting cost function\n",
    "W_val = []\n",
    "cost_val = []\n",
    "for i in range(-30, 50):\n",
    "    feed_W = i * 0.1\n",
    "    curr_cost, curr_W = sess.run([cost, W],\n",
    "                                feed_dict={W: feed_W})\n",
    "    W_val.append(curr_W)\n",
    "    cost_val.append(curr_cost)\n",
    "    \n",
    "# Show the cost function\n",
    "plt.plot(W_val, cost_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost 함수는 이렇게 생겼다! (= convex function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent algorithm - Tensor flow로 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](lec03.assets/image1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 step\n",
      "  cost: 26.7491397857666\n",
      "  W: [0.26279274]\n",
      "\n",
      "1 step\n",
      "  cost: 7.608643531799316\n",
      "  W: [0.60682285]\n",
      "\n",
      "2 step\n",
      "  cost: 2.164235830307007\n",
      "  W: [0.7903055]\n",
      "\n",
      "3 step\n",
      "  cost: 0.6156051158905029\n",
      "  W: [0.888163]\n",
      "\n",
      "4 step\n",
      "  cost: 0.17510537803173065\n",
      "  W: [0.9403536]\n",
      "\n",
      "5 step\n",
      "  cost: 0.04980776831507683\n",
      "  W: [0.9681886]\n",
      "\n",
      "6 step\n",
      "  cost: 0.014167515560984612\n",
      "  W: [0.9830339]\n",
      "\n",
      "7 step\n",
      "  cost: 0.00402988214045763\n",
      "  W: [0.9909514]\n",
      "\n",
      "8 step\n",
      "  cost: 0.00114628195296973\n",
      "  W: [0.9951741]\n",
      "\n",
      "9 step\n",
      "  cost: 0.0003260507364757359\n",
      "  W: [0.9974262]\n",
      "\n",
      "10 step\n",
      "  cost: 9.274231706513092e-05\n",
      "  W: [0.9986273]\n",
      "\n",
      "11 step\n",
      "  cost: 2.6379098926554434e-05\n",
      "  W: [0.9992679]\n",
      "\n",
      "12 step\n",
      "  cost: 7.503812867071247e-06\n",
      "  W: [0.99960953]\n",
      "\n",
      "13 step\n",
      "  cost: 2.134396027031471e-06\n",
      "  W: [0.99979174]\n",
      "\n",
      "14 step\n",
      "  cost: 6.070542326597206e-07\n",
      "  W: [0.9998889]\n",
      "\n",
      "15 step\n",
      "  cost: 1.7281445252592675e-07\n",
      "  W: [0.99994075]\n",
      "\n",
      "16 step\n",
      "  cost: 4.918531715247809e-08\n",
      "  W: [0.9999684]\n",
      "\n",
      "17 step\n",
      "  cost: 1.3994011283102736e-08\n",
      "  W: [0.9999832]\n",
      "\n",
      "18 step\n",
      "  cost: 3.967400630244811e-09\n",
      "  W: [0.99999106]\n",
      "\n",
      "19 step\n",
      "  cost: 1.112724135055032e-09\n",
      "  W: [0.99999523]\n",
      "\n",
      "20 step\n",
      "  cost: 3.183231456205249e-10\n",
      "  W: [0.99999744]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_data = [1,2,3]\n",
    "y_data = [1,2,3]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "X = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis for linear model W * X\n",
    "hypothesis = W * X\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_sum(tf.square(hypothesis - y))\n",
    "\n",
    "# Minimize: Gradient Descent using derivate:\n",
    "# W -= learning_rate * derivative\n",
    "learning_rate = 0.1  # ⍺\n",
    "gradient = tf.reduce_mean((W * X - y) * X)\n",
    "descent = W - learning_rate * gradient\n",
    "update = W.assign(descent)\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initialize global variables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(21):\n",
    "    _, _cost, _W = sess.run([update, cost, W], feed_dict={X: x_data,\n",
    "                                                   y: y_data})\n",
    "    print(f'{step} step\\n  cost: {_cost}\\n  W: {_W}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- assign : tensorflow 에서는 바로 assign 할 수 없어 W.assign(descent) 같은 방법으로 assign 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimize: Gradient Descent Magic (Lec 02.)\n",
    "\n",
    "optimizer = tf.train.<u>**GradientDescentOptimizer**</u>(learning_rate=0.1)  \n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GradientDescentOptimizer\n",
    "W 의 Initial Value 를 1000을 주고 실시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 step\n",
      "  W: 67.5999755859375\n",
      "\n",
      "1 step\n",
      "  W: 5.439994812011719\n",
      "\n",
      "2 step\n",
      "  W: 1.295999526977539\n",
      "\n",
      "3 step\n",
      "  W: 1.0197333097457886\n",
      "\n",
      "4 step\n",
      "  W: 1.0013155937194824\n",
      "\n",
      "5 step\n",
      "  W: 1.0000877380371094\n",
      "\n",
      "6 step\n",
      "  W: 1.000005841255188\n",
      "\n",
      "7 step\n",
      "  W: 1.0000003576278687\n",
      "\n",
      "8 step\n",
      "  W: 1.0\n",
      "\n",
      "9 step\n",
      "  W: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tf graph input\n",
    "X = [1,2,3]\n",
    "y = [1,2,3]\n",
    "\n",
    "# Set wrong model weights\n",
    "W = tf.Variable(1000.0)  \n",
    "# ↪ W 에 말도 안되는 값 넣어보고 Gradient Descent Algorithm 적용\n",
    "\n",
    "# Linear model\n",
    "hypothesis = W * X\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
    "\n",
    "# Minimize: Gradient Descent Magic\n",
    "optimizer = \\\n",
    "    tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initializes global variables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(10):\n",
    "    _W, _ = sess.run([W, train])\n",
    "    print(f'{step} step\\n  W: {_W}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional:  \n",
    "- **compute_gradient**\n",
    "- **apply_gradient****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 step\n",
      "  gradient: 9324.0\n",
      "  W: 1000.0\n",
      "  gvs: [(9324.0, 1000.0)]\n",
      "\n",
      "1 step\n",
      "  gradient: 621.5997924804688\n",
      "  W: 67.5999755859375\n",
      "  gvs: [(621.5998, 67.599976)]\n",
      "\n",
      "2 step\n",
      "  gradient: 41.4399528503418\n",
      "  W: 5.439994812011719\n",
      "  gvs: [(41.439953, 5.439995)]\n",
      "\n",
      "3 step\n",
      "  gradient: 2.762662172317505\n",
      "  W: 1.295999526977539\n",
      "  gvs: [(2.7626624, 1.2959995)]\n",
      "\n",
      "4 step\n",
      "  gradient: 0.18417732417583466\n",
      "  W: 1.0197333097457886\n",
      "  gvs: [(0.18417732, 1.0197333)]\n",
      "\n",
      "5 step\n",
      "  gradient: 0.012278874404728413\n",
      "  W: 1.0013155937194824\n",
      "  gvs: [(0.012278875, 1.0013156)]\n",
      "\n",
      "6 step\n",
      "  gradient: 0.0008188883657567203\n",
      "  W: 1.0000877380371094\n",
      "  gvs: [(0.00081888837, 1.0000877)]\n",
      "\n",
      "7 step\n",
      "  gradient: 5.4756801546318457e-05\n",
      "  W: 1.000005841255188\n",
      "  gvs: [(5.47568e-05, 1.0000058)]\n",
      "\n",
      "8 step\n",
      "  gradient: 3.0994415283203125e-06\n",
      "  W: 1.0000003576278687\n",
      "  gvs: [(3.0994415e-06, 1.0000004)]\n",
      "\n",
      "9 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "10 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "11 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "12 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "13 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "14 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "15 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "16 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "17 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "18 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "19 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "20 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "21 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "22 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "23 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "24 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "25 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "26 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "27 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "28 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "29 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "30 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "31 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "32 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "33 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "34 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "35 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "36 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "37 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "38 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "39 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "40 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "41 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "42 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "43 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "44 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "45 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "46 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "47 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "48 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "49 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "50 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "51 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "52 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "53 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "54 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "55 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "56 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "57 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "58 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "59 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "60 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "61 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "62 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "63 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "64 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "65 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "66 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "67 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "68 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "69 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "70 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "71 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "72 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "73 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "74 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "75 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "76 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "77 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "78 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "79 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "80 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "81 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "82 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "83 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "84 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "85 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "86 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "87 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "88 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "89 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "90 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "91 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "92 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "93 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "94 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "95 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "96 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "97 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "98 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n",
      "99 step\n",
      "  gradient: 0.0\n",
      "  W: 1.0\n",
      "  gvs: [(0.0, 1.0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tf graph input\n",
    "X = [1,2,3]\n",
    "y = [1,2,3]\n",
    "\n",
    "# Set wrong model weights\n",
    "W = tf.Variable(1000.0)\n",
    "\n",
    "# Linear model\n",
    "hypothesis = W * X\n",
    "\n",
    "# Manual gradient\n",
    "gradient = tf.reduce_mean((W * X - y) * X) * 2\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
    "\n",
    "# Minimize: Gradient Descent Magic\n",
    "optimizer = \\\n",
    "    tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "\n",
    "# <--- optimizer 바로 minimize 안하고, gradient 를 손 대보기\n",
    "# Get gradients\n",
    "gvs = optimizer.compute_gradients(cost, [W])\n",
    "\n",
    "# something to do ...\n",
    "\n",
    "# Apply gradients\n",
    "apply_gradients = optimizer.apply_gradients(gvs)\n",
    "# optimizer 바로 minimize 안하고, gradient 를 손 대보기 --->\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(100):\n",
    "    _gradient, _W, _gvs = sess.run([gradient, W, gvs])\n",
    "    print(f'{step} step\\n  gradient: {_gradient}\\n  W: {_W}\\n  gvs: {_gvs}\\n')\n",
    "    sess.run(apply_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gradient: 우리가 계산한 기울기\n",
    "- gvs: [GradientDescentOptimizer가 계산한 기울기, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
