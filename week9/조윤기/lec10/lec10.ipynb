{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN, ReLU, Xavier, Dropout, and Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-62c876dd6235>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/yoon/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/yoon/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /Users/yoon/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /Users/yoon/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/yoon/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/yoon/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holder\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "eval_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(eval_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch\n",
      "cost: 189.95067534359984\n",
      "2 epoch\n",
      "cost: 40.69706348419195\n",
      "3 epoch\n",
      "cost: 25.518892918500075\n",
      "4 epoch\n",
      "cost: 17.66576872719637\n",
      "5 epoch\n",
      "cost: 12.768214062914916\n",
      "6 epoch\n",
      "cost: 9.441962210876797\n",
      "7 epoch\n",
      "cost: 7.001015768898292\n",
      "8 epoch\n",
      "cost: 5.316174130113293\n",
      "9 epoch\n",
      "cost: 3.929858521558959\n",
      "10 epoch\n",
      "cost: 3.1391922833743067\n",
      "11 epoch\n",
      "cost: 2.2395012494217816\n",
      "12 epoch\n",
      "cost: 1.7819689082685644\n",
      "13 epoch\n",
      "cost: 1.3984012730495883\n",
      "14 epoch\n",
      "cost: 1.0590194065964662\n",
      "15 epoch\n",
      "cost: 0.9229023197005961\n",
      "\n",
      "accuracy: 0.9448999762535095\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {X: batch_x, Y: batch_y}\n",
    "            _cost, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "            avg_cost += _cost / total_batch\n",
    "\n",
    "        print(f'{epoch+1} epoch\\ncost: {avg_cost}')\n",
    "        \n",
    "    _acc = sess.run(accuracy, feed_dict={\n",
    "        X: mnist.test.images, Y: mnist.test.labels\n",
    "    })\n",
    "    print(f'\\naccuracy: {_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**기본 NN**</u>  <br>\n",
    "정확도 94%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드가 거의 똑같고, Xavier initialization 부분만 수정하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_variable() 사용 시, 동일한 변수명 겹치면 오류. reset 해줘야 한다.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holder\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 10],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "eval_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(eval_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch\n",
      "cost: 0.29900466988709845\n",
      "2 epoch\n",
      "cost: 0.11244006639515809\n",
      "3 epoch\n",
      "cost: 0.07478771491060879\n",
      "4 epoch\n",
      "cost: 0.052132528508928716\n",
      "5 epoch\n",
      "cost: 0.03782963317099283\n",
      "6 epoch\n",
      "cost: 0.03011943416945129\n",
      "7 epoch\n",
      "cost: 0.02396332610694859\n",
      "8 epoch\n",
      "cost: 0.016440701400173247\n",
      "9 epoch\n",
      "cost: 0.016857385337432123\n",
      "10 epoch\n",
      "cost: 0.01359818569109351\n",
      "11 epoch\n",
      "cost: 0.013620020609232592\n",
      "12 epoch\n",
      "cost: 0.010948073213142121\n",
      "13 epoch\n",
      "cost: 0.010185245019099317\n",
      "14 epoch\n",
      "cost: 0.011250503362018868\n",
      "15 epoch\n",
      "cost: 0.008528151728056732\n",
      "\n",
      "accuracy: 0.9797999858856201\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {X: batch_x, Y: batch_y}\n",
    "            _cost, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "            avg_cost += _cost / total_batch\n",
    "\n",
    "        print(f'{epoch+1} epoch\\ncost: {avg_cost}')\n",
    "        \n",
    "    _acc = sess.run(accuracy, feed_dict={\n",
    "        X: mnist.test.images, Y: mnist.test.labels\n",
    "    })\n",
    "    print(f'\\naccuracy: {_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Xavier initialization**</u><br>\n",
    "첫 1 epoch 의 cost 도 확 줄어서 시작하는 것을 알 수 있다. (초기값이 잘 설정됨)  <br>\n",
    "정확도 97% (3% 개선)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep NN for MNIST\n",
    "NN 을 더 wide 하고 deep 하게 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_variable() 사용 시, 동일한 변수명 겹치면 오류. reset 해줘야 한다.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holder\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "eval_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(eval_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch\n",
      "cost: 0.2908041570775891\n",
      "2 epoch\n",
      "cost: 0.10218646514483477\n",
      "3 epoch\n",
      "cost: 0.06963878090747373\n",
      "4 epoch\n",
      "cost: 0.05218298674053096\n",
      "5 epoch\n",
      "cost: 0.0421375889773481\n",
      "6 epoch\n",
      "cost: 0.03556778307178654\n",
      "7 epoch\n",
      "cost: 0.02824086396388751\n",
      "8 epoch\n",
      "cost: 0.02612629334910625\n",
      "9 epoch\n",
      "cost: 0.024941254039506682\n",
      "10 epoch\n",
      "cost: 0.01999805359167193\n",
      "11 epoch\n",
      "cost: 0.01920121032519754\n",
      "12 epoch\n",
      "cost: 0.017522784884310003\n",
      "13 epoch\n",
      "cost: 0.015909028814372013\n",
      "14 epoch\n",
      "cost: 0.017594474349911748\n",
      "15 epoch\n",
      "cost: 0.01457207557789595\n",
      "\n",
      "accuracy: 0.9782999753952026\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {X: batch_x, Y: batch_y}\n",
    "            _cost, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "            avg_cost += _cost / total_batch\n",
    "\n",
    "        print(f'{epoch+1} epoch\\ncost: {avg_cost}')\n",
    "        \n",
    "    _acc = sess.run(accuracy, feed_dict={\n",
    "        X: mnist.test.images, Y: mnist.test.labels\n",
    "    })\n",
    "    print(f'\\naccuracy: {_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Deep NN + Xavier**</u>  <br>\n",
    "정확도 97% (이전과 비슷)  <br>\n",
    "overfitting 이 발생해버렸다.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout for MNIST\n",
    "Regularization: Dropout 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-39-c1c31002feb6>:22: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# get_variable() 사용 시, 동일한 변수명 겹치면 오류. reset 해줘야 한다.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# dropout (keep_prob) rate\n",
    "# training 일때 0.7, testing 일때 1\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# input place holder\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "eval_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(eval_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch\n",
      "cost: 0.4533114428276366\n",
      "2 epoch\n",
      "cost: 0.173364374444566\n",
      "3 epoch\n",
      "cost: 0.12920449138026352\n",
      "4 epoch\n",
      "cost: 0.10840971099720753\n",
      "5 epoch\n",
      "cost: 0.09491395228119057\n",
      "6 epoch\n",
      "cost: 0.08269532063086948\n",
      "7 epoch\n",
      "cost: 0.07465190110304812\n",
      "8 epoch\n",
      "cost: 0.06793655089970504\n",
      "9 epoch\n",
      "cost: 0.06462515027346931\n",
      "10 epoch\n",
      "cost: 0.055708259453742984\n",
      "11 epoch\n",
      "cost: 0.056807905252049255\n",
      "12 epoch\n",
      "cost: 0.05455706898673353\n",
      "13 epoch\n",
      "cost: 0.04926481438652525\n",
      "14 epoch\n",
      "cost: 0.0450926262268331\n",
      "15 epoch\n",
      "cost: 0.04844479133152749\n",
      "\n",
      "accuracy: 0.9817000031471252\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {X: batch_x, Y: batch_y, keep_prob: 0.7}\n",
    "            _cost, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "            avg_cost += _cost / total_batch\n",
    "\n",
    "        print(f'{epoch+1} epoch\\ncost: {avg_cost}')\n",
    "        \n",
    "    _acc = sess.run(accuracy, feed_dict={\n",
    "        X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1\n",
    "    })\n",
    "    print(f'\\naccuracy: {_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Deep NN + Xavier + Dropout**</u>  <br>\n",
    "정확도 98% (이전에 비해 성능 개선이 이루어짐)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
